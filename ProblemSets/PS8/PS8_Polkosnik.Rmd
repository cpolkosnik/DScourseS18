---
title: "PS8"
author: "Conrad Polkosnik"
date: "3/26/2018"
output: word_document
---

Problem 4.
```{r}
library(nloptr)

# Random Number Generation
set.seed(100)

# Setting Variables for Random Generation
N=100000
K=10

# Random Number Generation for matrix, X
X <- matrix(rnorm(N*K,mean=0,sd=1), N, K)

# First matrix column with 1's
X[,1] <- 1

# Vector Length of N
eps <- rnorm(N, mean = 0, sd = 0.5)

head(eps)

beta  <- c(1.5, -1, 0.25, 0.75, 3.5, -2, 0.5, 1, 1.25, 2)

y <- X%*%beta + eps

head(y)
```

Problem 5.
```{r}
a <- (t(X)%*%X)

b <- solve(a)

c <- (t(X))

d <- b%*%c%*%y
d
```

Analysis:
The Estimate was nearly the exact same.

Problem. 6
```{r}
# Setting up gradient descent

# Step size
stepsize <- .0000003

# iterations
maxiter <- 500

# Objective function
objfunc <- function(beta, y, X) {
  return ( sum((Y-X%*%beta)^2))
}

# defining the gradient function f(x)
gradient <- function(beta, y, X) {
  return ( as.vector(-2*t(X)%*%(y-X%*%beta)) )
}

# initial values
beta <- runif(dim(X)[2])

# randomly initialize a value to beta
set.seed(100)

# create a vector to contain all beta's for all steps
beta.All <- matrix("numeric",length(beta),maxiter)

# gradient descent method to find the minimum
iter  <- 1
beta0 <- 0*beta

while (norm(as.matrix(beta0)-as.matrix(beta))>1e-8) {
    beta0 <- beta
    beta <- beta0 - stepsize*gradient(beta0,y,X)
    beta.All[,iter] <- beta
    if (iter%%10000==0) {
        print(beta)
    }
    iter <- iter+1
}

# print result and plot all xs for every iteration
print(iter)
print(paste("The minimum of f(beta,y,X) is ", beta, sep = ""))



```

Problem 7.
```{r}
# Nelder-Mead

library(nloptr)

# Objective function
objfun <- function(beta,y,X) {
return (sum((y-X%*%beta)^2))
# equivalently, if we want to use matrix algebra:
# return ( crossprod(y-X%*%beta) )
}

## Gradient of our objective function
gradient <- function(beta,y,X) {
return ( as.vector(-2*t(X)%*%(y-X%*%beta)) )
}

## initial values
beta0 <- runif(dim(X)[2]) #start at uniform random numbers equal to number of coefficients

## Algorithm parameters
options <- list("algorithm"="NLOPT_LD_LBFGS","xtol_rel"=1.0e-6,"maxeval"=1e3)

## Optimize!
result <- nloptr( x0=beta0,eval_f=objfun,eval_grad_f=gradient,opts=options,y=y,X=X)
print(result)

```

Analysis:
All of the values end up being basically the exact same.

Problem 8.
```{r}
library(nloptr)

# Objective function
objfun  <- function(theta,y,X) {

# need to slice our parameter vector into beta and sigma components
beta    <- theta[1:(length(theta)-1)]
sig     <- theta[length(theta)]
# write objective function as *negative* log likelihood (since NLOPT minimizes)
loglike <- -sum( -.5*(log(2*pi*(sig^2)) + ((y-X%*%beta)/sig)^2) ) 
return (loglike)
}

# initial values
theta0 <- runif(dim(X)[2]+1) #start at uniform random numbers equal to number of coefficients


# Algorithm parameters
options <- list("algorithm"="NLOPT_LN_NELDERMEAD","xtol_rel"=1.0e-6,"maxeval"=1e4)

# Optimization
result <- nloptr( 
x0=theta0,eval_f=objfun,opts=options,y=y,X=X)

print(result)

betahat  <- result$solution[1:(length(result$solution)-1)]
sigmahat <- result$solution[length(result$solution)]
```

Problem 9.
```{r}
# Computing OLS with lm()
EasyOLS <- lm(y~X -1)
EasyOLS
```

```{r}
# Stargazer Table
library("stargazer")
stargazer(EasyOLS)
```







